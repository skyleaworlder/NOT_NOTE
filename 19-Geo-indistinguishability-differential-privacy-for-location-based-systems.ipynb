{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geo-indistinguishability\n",
    "\n",
    "## info\n",
    "\n",
    "* Title: Geo-Indistinguishability: Diifferentail Privacy for Location-Based System\n",
    "* Miguel E. Andres\n",
    "\n",
    "* word：\n",
    "  * intuitive: 直觉的\n",
    "  * notion: 概念、观点、看法\n",
    "  * arrondissement: 这是法语，行政区\n",
    "  * perturbating: 摄动变分法\n",
    "  * planar: 平面的\n",
    "  * bandwidth overhead: 带宽开销\n",
    "  * negligible: 微不足道\n",
    "  * prohibitive: 贵的\n",
    "  * once and for all: 一劳永逸\n",
    "  * dummy localtion: 虚拟地址\n",
    "  * ubiquity: 无处不在\n",
    "  * cloak/cloaking: 遮盖掩盖的\n",
    "  * patch: 补丁、修补\n",
    "  \n",
    "## content\n",
    "\n",
    "Geo-indistinguishablility implies that the user is protected within any radius $r$, but with a level $l = \\epsilon r$ that increases with the distance.\n",
    "\n",
    "The experiments showed that the bandwidth overhead necessary to enhance LBS applications with very high levels of privacy and accuracy is not-prohibitive and, in most cases, negligible for modern applications.\n",
    "\n",
    "The user expects his location to be kept private: the information sent to the provider should not allow him to accurately infer the user's location.\n",
    "\n",
    "### K-anonymity\n",
    "\n",
    "About k-anonymity, many systems aim at protecting the user's identity, requiring that the attacker cannot infer which user is executing the query, among a set of $k$ different users. But this paper is interested in protecting the user's location.\n",
    "\n",
    "### Counter-measures\n",
    "\n",
    "Counter-measures, such as taking the environment into account might be complicated and simply be revealed by the attacker's actual side information.\n",
    "\n",
    "And Differential Privacy, quantization might be similar from my perspective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5.2\n",
    "print(int(a))\n",
    "\n",
    "from random import uniform\n",
    "delta = uniform(a=-0.5, b=0.5)\n",
    "a += delta\n",
    "print(int(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Privacy\n",
    "\n",
    "It requires that the probability that a query returns a value $v$ when applied to a database $D$, compared to the probability to report the same value when applied to an **adjacent** database $D'$-meaning that $D, D'$ differ in the value of a single individual - should be within a bound of $e^\\epsilon$.\n",
    "\n",
    "It can be successfully applied in cases where aggregate information about several users is published. On the other hand, the nature of this notion makes it poorly suitable for applications in which only a single individual is involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation approaches\n",
    "\n",
    "Transformation means that transforming all data to a different space, usually employing cryptographic techniques, so that they can be mapped back to spatial information only by user, thereby making it completely invisible to the service provider.\n",
    "\n",
    "We suppose that $K$ means that **a probabilistic function for selecting a reported value.** $K(x)(Z)$ means the probability that the reported point belongs to the set $Z \\subseteq Z'$. $\\pi(x)$ is the probability assigned to the location $x$.\n",
    "\n",
    "Each observation $Z \\subseteq Z'$ of a mechanism $K$ induces a posterior distribution\n",
    "\n",
    "$$\n",
    "\\sigma = Bayes(\\pi, K, Z)\\ on\\ X',\\ defined\\ as\\ \\sigma(x) = \\dfrac{K(x)(Z)\\pi(x)}{\\sum_{x'} K(x')(Z)\\pi(x')}\n",
    "$$.\n",
    "\n",
    "Define that $d_p(\\sigma_1, \\sigma_2) = sup_{S \\subseteq S'} |\\ln \\dfrac{\\sigma_1(S)}{\\sigma_2(S)}|$, with the convention that $|\\ln \\dfrac{\\sigma_1(S)}{\\sigma_2(S)}| = 0$ if both $\\sigma_1(S), \\sigma_2(S)$ are 0. $\\inf$ if only one of them is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo-indistinguishability\n",
    "\n",
    "$x, x'$ represent two **points**. Enjoying $l$-privacy within $r$ means that for any $x, x'$ s.t. $d(x, x') \\le r$，the distance $d_p(K(x), K(x'))$ between the corresponding distributions should be at most $l$.\n",
    "\n",
    "Then $\\epsilon r$-privacy for all radii $r$, forces the two distributions to be similar for **locations** close to each other, while **relaxing the constraint** for those far away from each other.\n",
    "\n",
    "Thus.\n",
    "\n",
    "Geo-indistinguishability means that **A mechanism $K$ satisfies $\\epsilon$-geo-indistinguishability if for all $x, x'$**:\n",
    "\n",
    "$$\n",
    "d_p(K(x), K(x')) \\le \\epsilon d(x, x')\n",
    "$$\n",
    "\n",
    "Equivalently, $K(x)(Z) \\le e^{\\epsilon d(x, x')} K(x')(Z),\\ for\\ all\\ x, x' \\in X, Z \\in Z'$\n",
    "\n",
    "Geo-indistinguishability is a generalized variant of differential privacy, using an arbitrary metric between secrets.\n",
    "\n",
    "This paper uses the Hamming metric of standard differential privacy, which aims at completely protecting the value of an individual - would be too strong. This paper are not going to hide the user's location completely. It aims at using a privacy level that depends on the Euclidean distance between locations is a natural choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterizations\n",
    "\n",
    "#### Hiding function\n",
    "\n",
    "Considerring a function $\\phi: X \\to X'$, it seems to be S-box in cryptography. Even if an adversary draws the same conclusions, it cannot figure out whether hiding has been applied or not. Of course, slight effect works well, instead of great affection.\n",
    "\n",
    "Maximum distance $d(\\phi) = sup_{x \\in X} d(x, \\phi(x))$.\n",
    "\n",
    "Then for all $\\phi: X \\to X'$, all priors $\\pi on X$, and all $Z \\subseteq Z':$\n",
    "\n",
    "$$\n",
    "d_p(\\sigma_1, \\sigma_2) \\le 2 \\epsilon d(\\phi),\\ where\\ \\sigma_1 = Bayes(\\pi, K, Z), \\sigma_2 = Bayes(\\pi, K \\circ \\phi, Z).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstracting from side information\n",
    "\n",
    "The goal of a privacy definition is to restrict the information **leakage** caused by the observation. Note that the lack of leakage does not mean that the user's location cannot be inferred, but instead that the adversary's knowledge does not increase due to the **observation**.\n",
    "\n",
    "Useful LBS query can infer some prior, like city, block and even exact location, to cause leakage caused by observation at a high probability.\n",
    "\n",
    "Differential privacy does ensure that her risk is the same whether she participates in the database or not, but this might be misleading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
