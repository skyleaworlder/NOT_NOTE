# 第五章-网络层

网络层是处理 **端到端** 的最底层。

对于我正在使用的计算机，它若要和另外一个计算机通信。中间的所有设备我都可以把它视作一个黑盒。从我看来，我好像就是直接连接了对方的计算机一样。

中间那个被我视为黑盒的事物，实际上是一系列的设备，这里面有交换机，也有路由器，当然还有其他可能的设备。这些都是 **ISP** 下的设备。它们（路由器）运行着某种算法，将我的消息传给另一方。

## 一、两种连接

### 1. 无连接服务过程

无连接 <===> 数据报网络。

每个路由器都维护一个表，这个表的 “键” 是 **`target`**，“值” 为 **下一跳路由器**。
对于网络内的每一个节点，在一个路由器内，都可能有一个下一跳路由器地址。

当机器打算发送一个数据报的时候，**ISP** 下的路由器们通过查阅自己维护的表，来将数据报一步步送到最后的目的地。在某本关于 “分布式算法” 的机工书里面被当作例子来精讲。

在这个过程中，路由表是一个关键角色。有关这个路由表，就诞生了以下问题：

1. 路由表怎么初始化 / 添加项 / 删除项？
2. 路由表是不是一成不变的，如果不是，该怎么变化？
3. 路由表是如何对数据报传递产生影响的？

其实还有很多问题，不过可能有些问题就不在计网的讨论范围之内了。

### 2. 有连接服务过程

就是固定好了一个，它不会改变路由表，写了就是写了。

但是在处理 **多主机连接路由器** 的时候也有一些问题，具体参见 **多协议标签交换 (MPLS)**。

## 二、路由算法

对路由算法的要求相当之高。问题来源于这些事实：

1. 大多数通信过程中间并不是只需要一个路由器就够了；
2. 网络拓扑结构可能经常改变；
3. 开发路由算法的时候，计算机正面临快速发展的时期；
4. 计算机若要走向普通人，那就需要让普通人觉得好用。

这蕴含了对很多特性的要求。比如第一个，中转节点一旦多了，设备的失效就可能必须纳入考虑范畴；而网络拓扑结构的易变，可能需要路由算法即插即用；普通人不懂什么交换，他只想要一个对自己而言稳定的网络环境……

总之要求是很多。

### 1. 单源最短路径 —— Dijkstra 算法

学离散应该学过。虽然离散距离现在已经一年半了，但我还清晰地记得当时离散最后一道题就考的 `Dijkstra Algorithm`。可惜的是，那个算法是我临考前 $5$ 个小时 “速通” 的，可以说考完了之后便一点印象都没有了。

现在，我今非昔比。相信可以用更易懂的语言来描述它。

#### i. Dijkstra 的精髓

个人以为是一个以 “距源长度” 为核心的 “伪优先队列”。（仅是个人观点）

而理解为什么它是精髓，就需要触碰算法最底层的逻辑。

#### ii. 先提一个暴论

```chinese
下一个操作对象是，那个 “距源长度” 最短的节点。
```

如果这个是对的，那么 “优先” 的概念呼之欲出。但为什么不是正经的 “优先队列” 呢？当然是因为在算法过程中，不需要 `enqueue`，所有节点一开始都在。不如说该数据结构是一个 “当其中某个元素的值遭到修改后，需要重排序” 的数组。

而这句话为什么对呢？我不会数学上的证明，只能从感觉上大概混一下：

试想，我现在是要寻找下一个操作对象，那么什么时候开始寻找呢？应该是 **操作过上一个节点之后**。按道理，我这句话是句废话。但在 `Dijkstra Algorithm` 中，操作节点意味着固定一个节点的 “距源长度”，相当于将那个点之前的 “距源长度” 视作单源最短路径长度。

而且可以从中得出一个结论：上一个操作节点，在已有的操作节点中，“距源长度” 是最大的；但曾经，它也是在未操作节点中 “距源长度” 最小的。（因为是一个一个从小到大选的）

比如书中的图 5-7(c) ：已操作节点的单源最短距离为 $[0, 2]$，未操作的自然是 $[4, 6, 9, +\infty, +\infty, +\infty]$。

那么按照算法，肯定选 $4$。并且我们需要明确一点：**任意操作节点，只能将未操作的节点的 “距源长度” 再度缩小，但永远不可能小于当前节点**。（而这是因为 `Dijkstra Algorithm` 的限制之一：“两节点间距离不能为负”，如果为负，那最小可能会失去意义，可能会出现负无穷的环路；同时，已经操作过的节点的 “距源距离” 很可能会变小）

并且可以得到一个结论：**已经选择的节点，不可能通过对剩余未操作节点的操作，再次降低 “距源长度”**。

也就是说我现在选了 $4$，可能会让后面的 $[6, 9, +\infty, +\infty, +\infty]$ 变小一些，变成 $[5, 6, 9, \infty, \infty]$，但绝不会变成 $[3, 6, 9, \infty, \infty]$。更不可能让前面的 $[0, 2]$ 变成 $[0, 1]$。

这像是一种分治思想。只不过这里交给了节点去 “自治” 属于自己的问题。和快排相似，只不过快排的 “属于自己的问题” 是当前数组中大于 / 小于选定基准值的两个元素数组而已。

#### iii. 程序

```phcode
def func Dijkstra(Graph G) {
    def Queue q

    sort(G.Nodes, G::Node::dist)
    q.enqueue(G.Nodes[0])
    while (!q.empty()) {
        G::Node n = q.dequeue()
        for (node in n.nextNodes) {
            if (G::hasOperated(node))
                continue;
            node.dist = (node.dist is +\infty) ? node.len : node.dist+node.len
        }
        sort(G.Nodes, G::Node::dist)
        if (G.Nodes.len is 0)
            q.enqueue(G.Nodes[0])
    }
}
```

### 2. 泛洪算法

唯一需要避免的是 **表无限膨胀**。

膨胀的原因是，路由器需要记录所有来源的数据报。泛洪本来就要发送大量的数据报，那么就非常有可能会把路由器挤爆。

做法有很多，最极端的自然是书上记录的这个方法：只记录收到的最大序列号，小于该序列号的全部丢弃。不过这需要源发方的配合了。

### 3. 距离矢量算法

想法是很美好，也是很容易理解的。

对于网络中的某一个节点，它有若干个邻接节点。该节点和它的邻接节点 **经常性互发路由表**。那么就可以通过 **路由表中的延迟** 以及 **互发消息的真实延迟** 计算出属于该节点的路由表。

这是有道理的。

* 首先，任意节点，想要与网络内的节点通信，必须首先经过它的邻接节点。
* 其次，关于邻接节点路由表中记录的延迟，该节点视之为 “可靠的”，也就是该节点不关心。
* 再次，该节点只关心自己和邻接节点交流路由表时的 **真实延迟**。
* 最后，如果假设传过来的路由表是对的，那么再加以自己保证正确的 **真实延迟**，那么最后得出的结论一定是正确的。

#### i. 但分布式总能带来些新花样

节点失效在所难免。

当某节点将自己的路由表交给它的临近节点后，如果它失效了该怎么办？

由于这个网络中不再有任何节点可以 `access` 那个节点，导致路由表中有关它的 “距离” 开始疯狂 `+1 +1 +1 +1...`。

在书中被称作 **无穷计算问题**。

### 4. 链路状态路由

与之前的分布式路由算法不同。这个虽然也是分布式算法，但是这个 “分布式” 具体体现在 **路由表的分发和接受** 上。

但到了计算的步骤时，却是在本地跑 `Dijkstra Algorithm`。而这就意味着 **链路状态路由需要每台路由器都有到整个网络中所有路由器的距离**。这也等价于 **每台路由器都要把自己的路由表发给其他所有路由器**。

算法一共分成五个部分：

1. 邻居节点的发现；
2. 设置与邻居节点的距离；
3. 链路数据包构造；
4. 将包发送给其余所有路由器，接受其余所有路由器的包；
5. 计算单源最短路径。

重点是第 4 点。该算法还是采用了泛洪法。那么泛洪法拥有的所有缺点，都需要在构造 **链路数据包** 的时候加以规避。

假设每个节点有 $k$ 个邻居节点，整个网络有 $n$ 个节点，那么显然每个节点在计算单源最短路径的时候，需要处理 $kn$ 大小的数据。如果节点数量膨胀，链路状态路由算法可能表现非常一般。

### 5. 层次路由

为了解决上面所说的 $n$ 过大，自然有了层次化想法。

有的时候分层是为了将问题分化，将原本复杂的问题分解为若干个正交的、有逻辑先后顺序的子问题。但有的时候却只是为了缩小问题范围，在所有范围内解决类似的问题。

层次路由也是这样。

如果是使用 “链路状态路由” 的话，原本需要考虑的是 “如果我网络里面有 $10000$ 个节点，我需要存 $10000$ 个路由项”。但若是将每 $1000$ 个路由划分为一个路由簇，岂不是每个机器只需要存 $(1000 + \dfrac{10000-1000}{1000})$ 个路由项了吗？

而不需要考虑原本那么多。分的越细，越是不需要考虑那么多路由项。

### 6. 广播路由

我也不太清楚，可能广播是路由器的本领。

假设我想要向 $A, B, C, D, E$ 发送数据报。当前这个路由器 $L$ 与三个路由器 $M, N, O$ 相连。且 $A, B, D$ 在 $M$ 下。$C, E$ 在 $N$ 下。

那么当前路由器 $L$ 会不理睬 $O$ 口，激活 $M, N$ 两个口，同时生成两个副本报文。一份报文中，广播的对象变成了 $A, B, D$ 而非原先的 $A, B, C, D, E$；另一份报文中变成了 $C, E$。

### 7. 其他路由

没有精力看了，以后再补吧。

## 三、拥塞控制

问题不是靠无脑增加资源就能解决的。

假设一个每秒处理 10 个包、内存也只能容纳 10 个数据包的路由器拥塞了。使它内存能够容纳 1000 个包并不是一个好方法。因为它的处理能力没有上去，届时会发送延迟过高的数据包。

但这里的意思不是说增加资源就不行。**增加资源** 和 **减少负载** 永远都是两类拥塞控制方法。

### 1. 朴素的拥塞控制思路

* 如果路由器能知道整体网络（周围局部网络）的拥塞状况就好了，这就可以随时间调整资源了；
* 如果路由器在拥塞的时候可以让源少发点就好了。

原本路由器只是做一个 **转发** 的工作而已。在讨论算法时，我们会少考虑很多实际情况，会去假设路由器有无限的内存，以及无限的处理能力。（当然，讨论算法时，算法会去优化存储相关事宜，但只是从理论数据量去考虑，而并非实际情况）

为完成第 1 个想法，那就需要 **感知流量**；而完成第 2 个需要路由器和源端持续通信。

### 2. 流量感知路由

路由器仅仅通过自己的负载来改变路由，是不好的。可能会引起 “路由振荡”。具体可见图 5-23。

### 3. 流量调节

假设现在有个路由器要炸了，它该如何告诉源端不要发那么快呢？

#### i. 最直接的想法

当然是直接告诉源端。通过发送一个 **抑制包**，减缓源端一定的发送流量。

#### ii. 让接受方转达

路由器在接收到源端发来的包时，把 **抑制标记** 打在包上，发给接受方。然后由接受方的响应包 `NOTIFY` 源端减小流量。

#### iii. 逐跳后压

接受方转达可能很慢，慢就是延迟，延迟就是更多未被控制的流量涌入路由器。

需要明确的是，**逐条后压也是让接受方转达**。但是每一条的路由器都变成了 **抑制标记** 的作用对象。原本 **抑制标记** 只是 `NOTIFY` 源端，而现在所有的中间路由器都要受到影响。

进而，流量并非从源端开始一步步减小，而是从倒数第一个路由器开始减小，**源端反而是最后一个减小流量的地方**。看下图 5-26

#### iv. 负载脱落

不玩了，直接躺平。无法处理的数据包就直接丢掉吧。

但丢包也是一种艺术，怎么丢？丢那个？对于不同的应用程序，有不同的丢包策略：

* `wine`：旧的比新的好；
* `milk`：新的比旧的好；
* 其他策略，比如谁出钱多，比如随机丢。

## 四、服务质量

后面会一直讨论这个东西，直到宇宙毁灭。

通常要传输的东西不是简简单单的一个数据包可以概括的，它会被拆成很多个数据包，这些数据包如果在交换网络中会散开来，最后汇聚到目标主机；如果是在面向连接的网络中，会沿着一条虚电路持续传输。对于这么一个过程，这么一系列数据包，会有一系列参数描述它的 **好 / 坏**。

* 带宽；
* 延迟；
* 抖动：延迟的变化 / 数据包到达时间的变化；
* 丢失。

这四者共同构成了描述 **服务质量** 的需求。

不同的网络服务对它们的需求是不一样的。当然，这里的 **需求** 有 **最低承受界限** 的意味。所有指标都拉满自然是最好的，但是当四者无法同时满足时，我们对于特定的服务，需要首先满足特定的指标。

### 1. 流量整形

有许多需要考虑的事情，首当其冲的是 “**协议**”。

这里的协议指的是：我怎么知道你这个服务提供方能否提供我想要的呢？

“这个实际还是很好办。既然是协议，那就和其他协议一样，交互个几次，确认几下就可以了。”

这种抽象的话自然是很好理解，也很好说。但需要注意，**流量整形** 的双方，一方是用户，另一方是网络提供方。发出质问的是用户，用户有何种何种应用程序，问网络提供方可否满足对应需求。详情可见 `SLA`。

### 2. 漏桶 / 令牌桶

这是两个思路。或者说是关于 “桶” 这个事物的两种描述。

#### i. 漏桶

重点在这个 “漏” 字儿。讲真的，无论是看书，还是看 `PPT`，它们都是假设你已经会了再去讲的。那一点概念都不知道的话，看那本书就简直像天书一样。

**“漏桶” 中，“请求” 就是桶里的 “水”**。来了多少请求都往桶里面装，装不下了就溢出来，不处理。但是一旦装进了桶里，迟早会从桶的漏口出流出。而 **“水流出” 就代表着 “请求被处理”**，放在这里就是 “路由器处理了发来的数据包”。

无论来的多凶猛，漏桶始终可以以小于、最多等于的速率处理请求。也就是说，**“漏桶” 能够平缓流量**。

#### ii. 令牌桶

令牌桶书上写的是一团乱，配图也非常糟糕。

**“令牌桶” 中的 “水” 是 “令牌”，而非 “请求”**（不然为啥叫令牌桶）。如果要处理到来的请求，就必须 **从令牌桶中获取令牌**，否则丢弃。往令牌桶中 “注水”，实际上就是在 “加令牌”，有了更多的 “令牌”，就允许处理更多的请求。

无论那一瞬间有多少请求，只要桶中还有令牌，都可以一下子处理。也就是说，**“令牌桶” 的最大处理能力是理论无穷的，可以处理突发流量**。

但令牌桶肯定也有一个最大处理速率。假设能够处理能力是 $M$，桶容量为 $B(bucket)$，令牌生成速率为 $R$，则有 **突发时间**，也就是 **最大处理速率的最长持续时间 $S$ 为：**

$$
B + RS = MS \Rightarrow S = \dfrac{B}{M-R}
$$

总结一下，漏桶和令牌桶，一个处理能力体现在 “漏口漏水速率”，一个体现在 “桶中令牌个数”。二者完全不是一回事。
